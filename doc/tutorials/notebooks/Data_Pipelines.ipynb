{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73403c5e-5d90-47a9-9e34-3e58570b0d98",
   "metadata": {},
   "source": [
    "# Data Module\n",
    "The data module helps you build complex data pipeline simply. Simply means that the code is readable and easily maintainable. This is due to the modularity of the data pipeline built with this module.\n",
    "\n",
    "## Overview\n",
    "The data pipeline builtd with this module can be built like a pipeline in the shell console.  They are composed of multiple sub-scripts that are agglomerated (piped) together with the pipe ```|``` operator.  This way, the sub-scripts can be reusable for multiple data pipes, and helps you build new data pipes quicker by composition of existing code.\n",
    "\n",
    "## Anatomy of a data pipeline\n",
    "The root class of the data pipelines is ```DataPipe```.  This class is a recursive and composable class.  This means that a single small script that does an elementary task is a ```DataPipe```, and an agglomeration (composition) of multiple elementary scripts is also a ```DataPipe```.  As it might be clear by now, a data pipeline is built by composition of multiple elementary pipes.  These elementary pipes are split into four categories: ```Fetch```, ```Process```, ```Collate```, ```Cache```. ```Fetch``` pipes fetch data from an external source.  It can be from the internet, from a database, from a file, etc.  ```Process``` pipes will transform the data from the upstream pipe.  It can be to change the datastructure, to impute nans, to filter the data, etc. ```Collate``` pipes are use to merge two branches of a pipeline.  For example, let's say you have a pipeline that fetch chart data from one data source, and another pipeline that fetch fundamental data formated as reports.  Each raw data needs to be processed differently, so they have their own subpipes.  However, they need to be agglomerated at the end to have a single dataset.  This is where a Collate pipe would comes in handy: it could align the two series together to merge the output of the two subpipes into one pipe.  Finally, the ```Cache``` pipe can cache the output of a data pipeline and prevent the wrapped pipeline section to run only when cache has been revalidated or expired.  Otherwise, it will return the cached data.  The default cache pipe supports multiple way of revalidating the cache."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba1292-8945-4d8b-8e47-61babdedb05d",
   "metadata": {},
   "source": [
    "## Basic Example\n",
    "The following example will show how to build a simple pipe that can fetch chart data based on a ticker and the yfinance api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c89fd877-ad65-44b0-ac2e-71182f554c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌ DataPipe(DataPipeType.FETCH, FetchCharts) ┐\n",
      "│                                           │\n",
      "│ FetchCharts                               │\n",
      "│                                           │\n",
      "└───────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Import the pipe\n",
    "from backtest.data import FetchCharts\n",
    "# Build the pipe\n",
    "pipe = FetchCharts([\"NVDA\"])\n",
    "# Print the representation of the pipe to have a clear view of what it will do.\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc306c8-1373-4ef4-bfa5-f3efc9178ee3",
   "metadata": {},
   "source": [
    "As we can see in the previous example, our pipe is constituted of a single elementary pipe which fetches the charts of the tickers given as parameters.  For now, the pipe is just built, it hasn't run.  To run it, we simply need to call the ```get``` method and pass the ```frm``` and ```to``` parameters, which are datetimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84bcebd0-7287-446f-bd87-436f4f93a82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of the output is:  <class 'dict'>\n",
      "The keys are:               dict_keys(['NVDA'])\n",
      "The type of the values are: <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "my_charts = pipe.get(frm=datetime(2022, 1, 1), to=datetime(2024,1,1))\n",
    "print(f\"The type of the output is:  {type(my_charts)}\")\n",
    "print(f\"The keys are:               {my_charts.keys()}\")\n",
    "print(f\"The type of the values are: {type(my_charts['NVDA'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d7ea8-62ee-4058-a5db-5d2eef33a934",
   "metadata": {},
   "source": [
    "---\n",
    "Let's complexify the example by adding elementary pipes to our pipe.  We will fetch the charts, ignore the charts that are None (Which means the asset didn't exist at the time requested), and impute the nan values with the previous value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ebc289e-5811-42e1-a827-bd023ce377d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TXYZ: No timezone found, symbol may be delisted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌ DataPipe(DataPipeType.PROCESS, CausalImpute) ───┐\n",
      "│                                                 │\n",
      "│ FetchCharts -> FilterNoneCharts -> CausalImpute │\n",
      "│                                                 │\n",
      "└─────────────────────────────────────────────────┘\n",
      "The type of the output is:  <class 'dict'>\n",
      "The keys are:               dict_keys(['NVDA', 'AAPL'])\n",
      "The type of the values are: <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "from backtest.data import FilterNoneCharts, CausalImpute\n",
    "\n",
    "pipe = FetchCharts([\"NVDA\", \"AAPL\", \"TXYZ\"]) | FilterNoneCharts() | CausalImpute()\n",
    "# Let's see what the pipe is doing\n",
    "print(pipe)\n",
    "\n",
    "# Let's fetch the data\n",
    "data = pipe.get(frm=datetime(2022, 1, 1), to=datetime(2024,1,1))\n",
    "print(f\"The type of the output is:  {type(data)}\")\n",
    "print(f\"The keys are:               {data.keys()}\")\n",
    "print(f\"The type of the values are: {type(data['NVDA'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9df322-ebb2-4061-87aa-bc38d797ba42",
   "metadata": {},
   "source": [
    "---\n",
    "As you can see, the pipe ```FilterNoneCharts``` ignored the 'TXYZ' asset because it doesn't exists, so it returned only the NVDA and AAPL charts.  If this elementary pipe wasn't there, the pipe would have return a key for 'TXYZ' that would map to a None.\n",
    "\n",
    "**Questions**  \n",
    "1.1 - What is the type of the FetchCharts pipe in the four types presented earlier?  \n",
    "1.2 - What is the type of the FilterNoneCharts pipe?\n",
    "\n",
    "Now, let's build a real-world pipe that you might find useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5d695b2-81fb-47a3-a637-501361902bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌ DataPipe(DataPipeType.CACHE, Cache) ───────────────────────────────────────────┐\n",
      "│                                                                                │\n",
      "│ FetchCharts -> FilterNoneCharts -> CausalImpute -> PadNan -> ToTSData -> Cache │\n",
      "│                                                                                │\n",
      "└────────────────────────────────────────────────────────────────────────────────┘\n",
      "The type of the output is:                     <class 'list'>\n",
      "The length of the list is:                     1\n",
      "The type of the element of the list is         <class 'dict'>\n",
      "The keys of the inner dict is                  dict_keys(['NVDA', 'AAPL', 'MSFT'])\n",
      "The type of the elements in the inner dict is: <class 'backtest.engine.tsData.TSData'>\n"
     ]
    }
   ],
   "source": [
    "from backtest.data import PadNan, ToTSData, Cache\n",
    "\n",
    "# Build the pipe\n",
    "pipe = FetchCharts([\"NVDA\", \"AAPL\", \"MSFT\"]) | FilterNoneCharts() | CausalImpute() | PadNan() | ToTSData() | Cache()\n",
    "print(pipe)\n",
    "\n",
    "# Fetch the data\n",
    "data = pipe.get(frm=datetime(2022, 1, 1), to=datetime(2024,1,1))\n",
    "print(f\"The type of the output is:                     {type(data)}\")\n",
    "print(f\"The length of the list is:                     {len(data)}\")\n",
    "print(f\"The type of the element of the list is         {type(data[0])}\")\n",
    "print(f\"The keys of the inner dict is                  {data[0].keys()}\")\n",
    "print(f\"The type of the elements in the inner dict is: {type(data[0]['NVDA'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414f9312-4023-45d3-b708-c60943a8f12f",
   "metadata": {},
   "source": [
    "---\n",
    "As you can see, we built a complex data structure with a single line of code.  As if it was not engough, it automatically cached the result, so we do not have to query the api anymore.  Now you might wonder what is this output data structure and why this pipe may be useful to you in the future.  Let's break it down!\n",
    "\n",
    "#### The output\n",
    "The final output is the data structure that the backtest object needs.  So, the output of this pipe is the input of the backtest.  More specifically, the backtest object needs the data to be formated as follow: A list of dictionnaries, where each index of the list correspond to a specific time resolution (our has a length of 1 because we only had a single time resolution which was 1 day, the default).  The dictionaries have string keys and TSData object values.  The keys are the tickers of the assets, and the values are the chart warpped by a TSData object which contains metadata information useful for the backtest engine.  Without the pipe, it would have been a tedious task to create a function that fetch the data, preprocess it, and format it in the good format.  However, building this pipeline with elementary pipes that are reusable is a charm!\n",
    "\n",
    "#### The pipes\n",
    "Let's break down what each pipe do.  The first three pipes have already been cover, so we will start with the 4th one.\n",
    "\n",
    "**PadNan**: This pipe will ensure that every timeseries (charts in our case) have the same length.  It will pad the start of the shorter charts with nan to ensure this.  It is required by the backtest object that each series have the same length.  \n",
    "**ToTSData**: This pipe converts a dictionary of DataFrames to the data structure presented above.  It is recommended to always use this pipe if the pipeline is meant to prepare the data for the backtest object.  \n",
    "**Cache**: Finally, the cache pipe caches the output of the pipe on the first run.  On the following runs, it is this pipe that is called first, and it will check if there is a cache file associated with this data.  If yes, it will skip all of the other pipes (from FetchCharts to Cache) and return the cached data.  If some other pipes would have beed added after the cache pipe, their data wouldn't have been cached.  Because we didn't passed any parameters to the cache pipe, it will be cached indefinitely.  However, if you change the pipe structure, or you change some values in the pipe (for example add a ticker to the list of tickers), it will automatically detect those changes and perform a full revalidation of the whole pipe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17408c40-1535-4662-b6f8-596fe97d5bcf",
   "metadata": {},
   "source": [
    "## Complex pipes\n",
    "It comes handy to have some prebuilt pipes for simple pipelines.  However, how can I build a complex pipeline fetching data from multiple sources, transforming each of them independently, and finally agglomerating everything together?  We'll see how to do this here.\n",
    "\n",
    "### Building custom elementary pipes\n",
    "As said earlier, there are four types of pipes, and each of them has some particularities to keep in mind when building a custom elementary pipe.  In this section, you will see how to create those custom pipes with examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21552f6-bab1-433a-b6a3-4d7169a23150",
   "metadata": {},
   "source": [
    "#### Fetch\n",
    "The fetch pipes are designed to fetch data from an external sources.  However, in this example, we will fetch the data from a global variable to simplify the examples.  There are two ways to build a custom fetch pipe.  The first one, and the simplest si from a decorator.  The second one is by deriving a class.  Here are the pros and cons of both methods:\n",
    "\n",
    "**Decorator**  \n",
    "Pros:\n",
    "- Functional approach\n",
    "- Simpler\n",
    "- Quicker to code, and less boiler plates\n",
    "\n",
    "Cons:\n",
    "- Cannot receives parameters during initialization\n",
    "- Cannot have a state.\n",
    "\n",
    "**Deriving a class**  \n",
    "Pros:\n",
    "- More flexibility\n",
    "- Can receives parameters during initialization\n",
    "- Can have a state\n",
    "\n",
    "Cons:\n",
    "- More boiler plates\n",
    "- Might be longer to code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea9a6bf-b94c-425b-b327-41bc3fb8de2b",
   "metadata": {},
   "source": [
    "To make a custom pipe using the **decorator** technique, you only need to add the ```@Fetch``` at the top of your function.  Teh function must have a particular signature.  It must have as positional parameters the ```frm``` and ```to``` paramter, which are datetimes.  This means that your pipe should always fetch data between those two datetime to avoid unexepected results.  It pust also have the po parameters, which is a keyword parameter that receives a ```PipeOutput```.  The PipeOuput objects corresponds to the output of the previous pipe.  If the current pipe is the first of the pipeline, the po parameter will be ```None```.  You function must also take as parameters positional arguments that can be passed to the pipeline and keyword arguments.  That's why the ```*args``` and ```**kwargs``` are added.  It will be explained in more details later why this is required.  Finallym if we print the type of the function, we can find out it isn't a function, but a Fetch object.  We built a custom DataPipe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ada284e-b712-49a1-b0be-4580a0f3f436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of FetchDec is: <class 'backtest.data.pipes.Fetch'>\n",
      "\n",
      "┌ DataPipe(DataPipeType.FETCH, FetchDec) ┐\n",
      "│                                        │\n",
      "│ FetchDec                               │\n",
      "│                                        │\n",
      "└────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "from backtest.data import Fetch\n",
    "\n",
    "MY_VALUES = [1.618, 2.71828, 3.1416, 42.]\n",
    "\n",
    "@Fetch\n",
    "def FetchDec(frm, to, *args, po, **kwargs):\n",
    "    return MY_VALUES\n",
    "\n",
    "print(f\"The type of FetchDec is: {type(FetchDec)}\\n\")\n",
    "print(FetchDec())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffb8514-11cf-4a41-9626-9fa22f90a0bb",
   "metadata": {},
   "source": [
    "---\n",
    "Now, we will build a custom pipe using the **class derivation** technique.  To do so, you need to derive a class from the DataPipe base class (The fetch is only made as a decorator, and it isn't recommended to derive from it).  To do so, we need to override the constructor and the fetch method.  The constructor can take as many parameters as we like.  We also need to initialize the super class by passing the pipe type and the name of the pipe. It is recommended to use the same name as the name of the class.  Then, we can override the fetch method and implement the logic there, like we did with the decorator technique.  However, it must return a PipeOutput, not any object like in the decorator technique.  In addition, the pipe output must be refrence to the current object 'self'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b3e763e-b4bf-4352-8d58-ae1730eca286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of FetchClf is: <class '__main__.FetchClf'>\n",
      "\n",
      "┌ DataPipe(DataPipeType.FETCH, FetchClf) ┐\n",
      "│                                        │\n",
      "│ FetchClf                               │\n",
      "│                                        │\n",
      "└────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "from backtest.data import DataPipe, DataPipeType, PipeOutput\n",
    "\n",
    "class FetchClf(DataPipe):\n",
    "    def __init__(self, my_param=\"I can receive params!\"):\n",
    "        super().__init__(DataPipeType.FETCH, \"FetchClf\")    # PipeType, Pipe Name\n",
    "        self.my_param = my_param\n",
    "\n",
    "    def fetch(self, frm, to, *args, po, **kwargs):\n",
    "        print(self.my_param)\n",
    "        return PipeOutput(MY_VALUES, self)\n",
    "\n",
    "print(f\"The type of FetchClf is: {type(FetchClf())}\\n\")\n",
    "print(FetchClf())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df1ec8e-a06a-49d1-8f04-986b3a182d2a",
   "metadata": {},
   "source": [
    "#### Process\n",
    "To make custom process pipes, it is the same as Fetch pipes, but using the Process decorator, or the Process data pipe type and overriding the process method instead of the fetch for the class deriving method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5fc82c-7228-4987-b25c-e1fb4907cdc4",
   "metadata": {},
   "source": [
    "#### Collate\n",
    "There is still two ways to make a custom collate pipe.  The first one is the **decorator**.  It is similar to the Fetch and the Process sunthax, but it takes two pipe output as input: ```po1```, ```po2```.  In the following example, we make a Collate pipe that assumes that the values of the pipe outputs are lists and concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5eb3df5-f371-41a0-985b-3f40324f3b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of CollateDec is: <class 'backtest.data.pipes.Collate'>\n",
      "\n",
      "┌ DataPipe(DataPipeType.COLLATE, CollateDec) ┐\n",
      "│                                            │\n",
      "│ FetchDec -> ┐                              │\n",
      "│             │ -> CollateDec                │\n",
      "│ FetchClf -> ┘                              │\n",
      "│                                            │\n",
      "└────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "from backtest.data import Collate\n",
    "\n",
    "@Collate\n",
    "def CollateDec(frm, to, *args, po1, po2, **kwargs):\n",
    "    return po1.value + po2.value\n",
    "\n",
    "print(f\"The type of CollateDec is: {type(CollateDec)}\\n\")\n",
    "print(CollateDec(FetchDec(), FetchClf()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c184b9da-b970-4263-9941-ed6243021394",
   "metadata": {},
   "source": [
    "---\n",
    "The **class derivation** method is trickier because we need to manually handle the pipe ids, which we haven't seen yet.  So, we will make an example, and there is a part of the code that will be explained later on. Like for a Fetch pipe, we need to derive from the DataPipe class and initialize the super class.  We need to use the COLLATE data pipe type.  We also need to register the two branches in a list of two pipe called ```_pipes```.  It must be called like this.  With this line, the DataPipe will be able to handle both branches under the hood and pass their output to the collate method.  Finally, we can override the collate method and implement it like with the decorator technique.  However, note that it must return a PipeOutput referenced to the current object ('self')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d77f9ee-d070-4cfb-8502-8c79e8f91656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of CollateClf is: <class 'abc.ABCMeta'>\n",
      "\n",
      "┌ DataPipe(DataPipeType.COLLATE, CollateClf) ┐\n",
      "│                                            │\n",
      "│ FetchDec -> ┐                              │\n",
      "│             │ -> CollateClf                │\n",
      "│ FetchClf -> ┘                              │\n",
      "│                                            │\n",
      "└────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "class CollateClf(DataPipe):\n",
    "    def __init__(self, pipe1, pipe2):\n",
    "        super().__init__(DataPipeType.COLLATE, \"CollateClf\")\n",
    "        # We register the two branches in order for the DataPipe to handle them.\n",
    "        self._pipes = [pipe1, pipe2]\n",
    "        \n",
    "        # Technical line explained later\n",
    "        self._pipe_id = pipe2._increment_id(pipe1._pipe_id + 1)\n",
    "\n",
    "    def collate(self, frm, to, *args, po1, po2, **kwargs):\n",
    "        return PipeOutput(po1.value + po2.value, self)\n",
    "\n",
    "print(f\"The type of CollateClf is: {type(CollateClf)}\\n\")\n",
    "print(CollateClf(FetchDec(), FetchClf()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19127135-7199-44c4-974d-68db1d7edb64",
   "metadata": {},
   "source": [
    "#### Putting everythng together\n",
    "In the following example, we will see how our implementation using the decorator technique and the class derivation techniques gives the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72dcb9ea-759b-4739-a46d-1cfe676a812e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌ DataPipe(DataPipeType.COLLATE, CollateDec) ┐\n",
      "│                                            │\n",
      "│ FetchDec -> ┐                              │\n",
      "│             │ -> CollateDec                │\n",
      "│ FetchDec -> ┘                              │\n",
      "│                                            │\n",
      "└────────────────────────────────────────────┘\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "I can receive params!\n",
      "I can receive params!\n",
      "┌ DataPipe(DataPipeType.COLLATE, CollateClf) ┐\n",
      "│                                            │\n",
      "│ FetchClf -> ┐                              │\n",
      "│             │ -> CollateClf                │\n",
      "│ FetchClf -> ┘                              │\n",
      "│                                            │\n",
      "└────────────────────────────────────────────┘\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Decorator: [1.618, 2.71828, 3.1416, 42.0, 1.618, 2.71828, 3.1416, 42.0]\n",
      "Class derivation: [1.618, 2.71828, 3.1416, 42.0, 1.618, 2.71828, 3.1416, 42.0]\n"
     ]
    }
   ],
   "source": [
    "pipe_dec = CollateDec(FetchDec(), FetchDec())\n",
    "dec_out = pipe_dec.get(None, None)\n",
    "print(pipe_dec)\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "pipe_clf = CollateClf(FetchClf(), FetchClf())\n",
    "clf_out = pipe_clf.get(None, None)\n",
    "print(pipe_clf)\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "print(f\"Decorator: {dec_out}\")\n",
    "print(f\"Class derivation: {clf_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abca7535-23e9-427c-9401-d2b6c1bec176",
   "metadata": {},
   "source": [
    "---\n",
    "In the previous example, we can see that both methods gives the same results.  We can see the string 'I can receive params!' printed two times because the FetchClf object is called twice, and we built the pipe in a way to print this each time it is called."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d1ad30-769c-4c24-b8ee-ff1eda054397",
   "metadata": {},
   "source": [
    "## Deep dive into Caching\n",
    "Implementing robust caching techniques from scratch can be tedious.  This is why the datapipe api comes with a prebuilt caching support.  It can support by default simple caching and revalidating mechanism, and can be extended to handle any arbitrarly complex caching and revalidating mecanism.  There is again two ways to extend the caching mechanism.  The first one is the simplest, but the less elegant: using a combination of decorator and callbacks.  If it is what you want to do, I recommend you to take a look at the example in the docs (Cache object).  The other way is by deriving the class.  We won't enter into the technical details on how to make a custom caching mecanism in this tutorial since most people won't need to do so.  If you need to understand those mecanism, I suggest you to read the docs on the Cache object and check the implementation of the JSONCache object in the source code.  I believe it is well documented and can be a great example.\n",
    "\n",
    "Now, coming back to the tutorial, we take a look at the two prebuilt caching pipes.  We will look at how to use them, and a brief overview of how they work under the hood.\n",
    "\n",
    "### How to interprete a pipe using caching\n",
    "When a cache pipe is added to a datapipe, it automatically wraps everything to the left of of it.  This means that this pipe\n",
    "```fetch() | process() | cache()``` is equivalent to do this in a functional notation: ```cache(process(fetch()))```.  In this example, the output of the process is cached and the process and fetch pipes will not be run again will the cache is still valid.\n",
    "\n",
    "### Cache and JSONCache\n",
    "There is two types of prebuilt cache pipes: ```Cache``` and ```JSONCache```.  The first one caches the data in a pickle format.  It is more flexible than  JSON and can result in a smaller file size for big objects.  However, it is harder to inspect the file and understand what data is stored compared to a text file such as JSON.  This is why there is another cache pipe that stores the cache in JSON: ```JSONCache```.  By default, JSON is pretty limited to basic datatypes such as float, strings, bool, etc.  This is why we extended the JSON serializing mechanism to handle more complex datatypes and be easily extendable to other datatypes.  It supports py default multiple complex datatype that you could use such as pandas DataFrames and numpy arrays.  Most objects are also serializable by default.  However, if you want a specific way to serialize your custum object, check out this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0be0bfd-599e-4867-82a4-50d31d026502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"__TYPE__\": \"MyClass\", \"data\": {\"a\": 1, \"b\": 2}}\n",
      "MyClass(1, 2)\n",
      "\n",
      "{\"__TYPE__\": \"MyClass2\", \"data\": {\"a\": 1, \"b\": 2}}\n",
      "MyClass2(1, 2)\n"
     ]
    }
   ],
   "source": [
    "from backtest.data import json_extension as je\n",
    "import json\n",
    "\n",
    "class MyClass:\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "    \n",
    "    def __tojson__(self):\n",
    "        return {\"a\": self.a, \"b\": self.b}\n",
    "    \n",
    "    @classmethod\n",
    "    def __fromjson__(cls, d):\n",
    "        return cls(d[\"a\"], d[\"b\"])\n",
    "    def __repr__(self):\n",
    "        return f\"MyClass({self.a}, {self.b})\"\n",
    "    \n",
    "je.add_types(MyClass)\n",
    "obj = MyClass(1, 2)\n",
    "print(json.dumps(obj, cls=je.JSONEncoder))\n",
    "# To deserialize the object\n",
    "d = '{\"__TYPE__\": \"MyClass\", \"data\": {\"a\": 1, \"b\": 2}}'\n",
    "print(json.loads(d, cls=je.JSONDecoder))\n",
    "# To unregister a type, use the remove_types function\n",
    "je.remove_types(MyClass)\n",
    "\n",
    "print() # For spacing in th output\n",
    "# However, in this case, it wasn't necessary to implement a custom serializer because the serializer can handle it by default:\n",
    "class MyClass2:\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"MyClass2({self.a}, {self.b})\"\n",
    "\n",
    "obj = MyClass2(1, 2)\n",
    "print(json.dumps(obj, cls=je.JSONEncoder))\n",
    "# To deserialize the object\n",
    "d = '{\"__TYPE__\": \"MyClass2\", \"data\": {\"a\": 1, \"b\": 2}}'\n",
    "print(json.loads(d, cls=je.JSONDecoder))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d649a3-900d-43f0-a613-e09d28328057",
   "metadata": {},
   "source": [
    "---\n",
    "The following shows an example of registering a custom class with the JSONCache pipe.  Its basically the same synthax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "286b2a96-63d8-45d8-8d35-da37e23effc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyClass(1, 2)\n",
      "MyClass(1, 2)\n"
     ]
    }
   ],
   "source": [
    "from backtest.data import Fetch, JSONCache\n",
    "\n",
    "# Register the class.  (Usually not necessary, but we show it for the pupose of the tutorial)\n",
    "je.add_types(MyClass)\n",
    "\n",
    "OBJ = MyClass(1, 2)\n",
    "\n",
    "@Fetch\n",
    "def FetchNum(frm, to, *args, po, **kwargs):\n",
    "    return OBJ\n",
    "\n",
    "pipe = FetchNum() | JSONCache()\n",
    "\n",
    "# Fisrt run, it isn't cache\n",
    "print(pipe.get(None, None))\n",
    "\n",
    "OBJ = MyClass(42, 42)\n",
    "\n",
    "# Second run: It is cached and loaded fro cache\n",
    "print(pipe.get(None, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669ed2f-54cc-4be2-8547-33737428f273",
   "metadata": {},
   "source": [
    "### Overview of the mecanism\n",
    "The great question: 'How does it work under the hood?'  \n",
    "- On the first run, it runs after the previous pipes and store the output in memory.  If the ```store``` parameter is set to True, it is stored on disk in a pickle file or a json file depending on the caching pipe you chose or with the ```caching_cb``` passed as parameter.  \n",
    "- On the following runs, the cache pipe is called first (before the pipes it is wrapping). If stored the cache is stored, it loads the cache from the disk using the default loading mecanism or the ```loading_cb``` if provided.  Then, it verifies if the cache is still valid.  By default, it will verify if the cache is not too old (Stored datetime + ```timeout``` parameter) or if it hasn't been hit more times than ```max_requests```.  If a ```revalidate_cb``` is provided, it is called to determine of the cache is still valid.\n",
    "- If the cache is not valid anymore, it is revalidated and the part of the pipe wrapped inside the cache object is run again.\n",
    "- If the cache is still valid, it returns the cached data.\n",
    "- If the pipe has changed: its structure or its pipe's parameters, the cache object will detect it and it will trigger a full revalidation.  This means  that every cache object will revalidate their cache.  In other words, all the cache will be revalidated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8928342c-c42e-4b40-97a6-9de17b4d52f7",
   "metadata": {},
   "source": [
    "## Technicality of DataPipe and Pipe Ids\n",
    "\n",
    "This section explains the technicalities of how the datapipes are built under the hood.  During the build process, each elementary pipe is given a unique id that is unique inside the pipe.  However, because the pipe can be built in multiple steps, the given ids aren't fixed utils the pipe is run.  This means that while a pipe isn't run, any ids can be changed internally.  However, once the pipe is run, the ids becomes fixed and cannot be changed anymore.  We call this process a pipe forging.  This is because a pipe is flexible before it is run, but becomes fixed and cannot be changed after being run.  Also, during the forgin process, the pipe ids may change to become unique accross all pipes, not only the current pipe.  Let's see an example:\n",
    "```python\n",
    "pipe1 = fetch() | process()\n",
    "pipe1.get(datetime(2020, 1, 1), datetime(2021, 1, 1))    # Forge the pipe to reserve the ids\n",
    "pipe2 = fetch() | process()\n",
    "pipe2.get(datetime(2020, 1, 1), datetime(2021, 1, 1))    # Forge the pipe to get the real ids\n",
    "```\n",
    "In the previous code, the first pipe *could have* the following ids: (fetch: 1, process: 2) and the second (fetch: 3, process: 4).  It is precised 'could have' because it depends on what pipes were built previously.  Like in that example, even though both pipes are identical, they have different ids.  However, if the get method wouldn't have been called, their ids would have been the same *i.e.* (fetch: 1, process: 2).  This is because the pipe wouldn't have been forged and the ids would be unique for their pipe, but not for every pipes.\n",
    "\n",
    "### Why it matters?\n",
    "Usually, you won't even need to bother with the pipe ids.  However, there might have some situation were it could be useful to understand the concept.  For example: when using caching in jupyter notebooks.  This is because the caching pipes uses their pipe_id to identify the cache file and revalidate its cache.  If the pipe_id of a cache pipe changes, it might use the cache of another pipe, which could cause bugs.  Usually, this would not happen because each pipe is built deterministically in the same order in a script thus inheriting deterministically the same pipe_id.  However, it is not the case in jupyter notebooks where cells can be run in different orders depending on the user's intentions.  This means that the same pipeline could inherit different ids depending on how the user run the notebook.  This can cause problem if the pipe uses caching.  To tackle this prolem, there is a method called ```set_id``` that will set the ids of the pipe and forge it in order to fix the ids.  Because they are not automatically assigned, there is no verification done to check if the ids are unique.  (They are unique inside the pipe, but could overlap with the ids of another pipe.)  This being said, you must make sure the assigned ids are unique.  A good thumb rule is to first run your notebook with automatically assigned ids, then specify the same ids in the ```set_id``` method.  \n",
    "\n",
    "**Example**:  \n",
    "During the first run, we do not set any pipe ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e5cdb7e-848a-4b0f-9472-496bfc22e878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "FetchCharts: 0\n",
      "FilterNoneCharts: 1\n",
      "CausalImpute: 2\n",
      "PadNan: 3\n",
      "ToTSData: 4\n",
      "====================================================================================================\n",
      "FetchCharts: 5\n",
      "FilterNoneCharts: 6\n",
      "CausalImpute: 7\n",
      "PadNan: 8\n",
      "ToTSData: 9\n"
     ]
    }
   ],
   "source": [
    "from backtest.data import PadNan, ToTSData, CausalImpute, FilterNoneCharts, FetchCharts\n",
    "from datetime import datetime\n",
    "\n",
    "# Build the pipe\n",
    "pipe1 = FetchCharts([\"NVDA\", \"AAPL\", \"MSFT\"]) | FilterNoneCharts() | CausalImpute() | PadNan() | ToTSData()\n",
    "pipe1.get(datetime(2020, 1, 1), datetime(2021, 1, 1))   # Run to forge the pipe\n",
    "pipe2 = FetchCharts([\"NVDA\", \"AAPL\", \"MSFT\"]) | FilterNoneCharts() | CausalImpute() | PadNan() | ToTSData()\n",
    "pipe2.get(datetime(2020, 1, 1), datetime(2021, 1, 1))   # Run to forge the pipe\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "for p in pipe1:\n",
    "    print(f'{p.name}: {p.pipe_id}')\n",
    "print(\"=\"*100)\n",
    "for p in pipe2:\n",
    "    print(f'{p.name}: {p.pipe_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a634b04d-5d90-487d-bf82-67c86801f289",
   "metadata": {},
   "source": [
    "Then, we change the cells to add the ```set_id()``` method call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ab43523-1d5f-4bda-916f-569e75109cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "FetchCharts: 0\n",
      "FilterNoneCharts: 1\n",
      "CausalImpute: 2\n",
      "PadNan: 3\n",
      "ToTSData: 4\n",
      "====================================================================================================\n",
      "FetchCharts: 5\n",
      "FilterNoneCharts: 6\n",
      "CausalImpute: 7\n",
      "PadNan: 8\n",
      "ToTSData: 9\n"
     ]
    }
   ],
   "source": [
    "from backtest.data import PadNan, ToTSData, CausalImpute, FilterNoneCharts, FetchCharts\n",
    "from datetime import datetime\n",
    "\n",
    "# Build the pipe\n",
    "pipe1 = FetchCharts([\"NVDA\", \"AAPL\", \"MSFT\"]) | FilterNoneCharts() | CausalImpute() | PadNan() | ToTSData()\n",
    "pipe1.set_id(0)    # Always pick the smallest id\n",
    "pipe1.get(datetime(2020, 1, 1), datetime(2021, 1, 1))   # Run to forge the pipe\n",
    "pipe2 = FetchCharts([\"NVDA\", \"AAPL\", \"MSFT\"]) | FilterNoneCharts() | CausalImpute() | PadNan() | ToTSData()\n",
    "pipe2.set_id(5)\n",
    "pipe2.get(datetime(2020, 1, 1), datetime(2021, 1, 1))   # Run to forge the pipe\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "for p in pipe1:\n",
    "    print(f'{p.name}: {p.pipe_id}')\n",
    "print(\"=\"*100)\n",
    "for p in pipe2:\n",
    "    print(f'{p.name}: {p.pipe_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e339e-90a8-46b8-a11e-d073038efae9",
   "metadata": {},
   "source": [
    "---\n",
    "Now, you can try to run mulitple time the first cell, you will see that the ids keep incrementing defining new pipes at every run.  However, in the second cell, no matters how many times you run the cell, the pipe ids will stay the same.  It won't create new pipes, it will just re-initialize the pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3c6fdf-cf7f-4149-82f7-1fbe33da3a55",
   "metadata": {},
   "source": [
    "## Question Answers\n",
    "1.1: Fetch  \n",
    "1.2: Process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
